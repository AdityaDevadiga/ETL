{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "926b9a85-b859-42e4-afa9-a4b10b26d73a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records in dataset: 995\nNumber of rows with null or empty cells: 1\nNumber of columns with null or empty cells: 1\nNumber of rows with all columns having values: 994\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"YouTubeStatisticsAnalysis\").getOrCreate()\n",
    "\n",
    "# Set AWS credentials for Spark session\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"**************\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"****************\")\n",
    "\n",
    "# Read CSV data from S3 bucket\n",
    "bucket_name = 'your_bucket_name'\n",
    "object_key = 'object_key_name'\n",
    "s3_uri = f\"s3a://{bucket_name}/{object_key}\"\n",
    "\n",
    "# Read CSV data into a PySpark DataFrame\n",
    "df = spark.read.csv(s3_uri, header=True, inferSchema=True)\n",
    "\n",
    "# Count the total number of rows in the dataset\n",
    "num_of_rows = df.count()\n",
    "\n",
    "# Count the number of rows with null or empty cells\n",
    "num_rows_with_null = df.rdd.filter(lambda row: any(cell is None or cell == '' for cell in row)).count()\n",
    "\n",
    "# Count columns with null or empty cells\n",
    "num_columns_with_null = sum([df.filter(col(col_name).isNull() | (col(col_name) == '')).count() > 0 for col_name in df.columns])\n",
    "\n",
    "# Count the number of rows with all columns having values\n",
    "num_complete_rows = df.dropna().count()\n",
    "\n",
    "# Add a new column that counts non-null values in each row\n",
    "non_null_cols = [col(col_name) for col_name in df.columns]\n",
    "non_null_counts = [col(c).isNotNull().cast('integer').alias(f'{c}_non_null_count') for c in df.columns]\n",
    "df = df.select(*df.columns, *non_null_counts)\n",
    "\n",
    "# Add a new column that counts null values in each row\n",
    "null_counts = [col(c).isNull().cast('integer').alias(f'{c}_null_count') for c in df.columns]\n",
    "df = df.select(*df.columns, *null_counts)\n",
    "\n",
    "# Add a new column that counts null values (empty cells) in each row\n",
    "null_or_empty_counts = [((col(c).isNull() | col(c).contains('')).cast('integer')).alias(f'{c}_null_or_empty_count') for c in df.columns]\n",
    "df = df.select(*df.columns, *null_or_empty_counts)\n",
    "\n",
    "# Print results\n",
    "print(\"Total number of records in dataset:\", num_of_rows)\n",
    "print(\"Number of rows with null or empty cells:\", num_rows_with_null)\n",
    "print(\"Number of columns with null or empty cells:\", num_columns_with_null)\n",
    "print(\"Number of rows with all columns having values:\", num_complete_rows)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "task3_etl",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
